##1.1 深度学习加速器

<p class="content">深度学习（Deep Learning）以人工神经网络为基础架构，是机器学习领域的一个重要分支。2006年，著名科学家Hinton首次提出了深度学习的概念，其利用预训练方法缓解了局部最优解问题，将神经网络的训练推动到7层，实现了神经网络真正意义上的“深度”。2015年，深度学习三位顶级专家 Lecun, Bengio, Hinton合作在Nature合作发表著名论文“Deep Learning (深度学习)”。这篇论文正式标志着，以深度神经网络为代表的深度学习技术正式走向人工智能研究的中央C位。这三位杰出的科学家也因为深度神经网络方面的研究在2019年获得了图灵奖。鉴于当前深度学习研究和应用中主要使用深度神经网络，本文不显著区分深度学习与深度神经网络，本文所指深度学习加速器也被称为神经网络加速器。</p>


<p class="content">诸多研究表明，深度学习对于解决现实世界的问题非常有效，已经广泛应用于各个领域。如图1-1所示，当前深度学习算法已经大规模应用于云服务、生物医学、媒体娱乐、安全防御、自动驾驶等领域。在云服务领域，深度学习算法可完成图像分类与语音识别、语言翻译与处理、情绪分析推荐等任务。在生物医学领域，可以完成癌细胞检测、糖尿病分级和药物发现等任务。在媒体娱乐方面，视频字幕、视频搜索和实时翻译等也可使用深度学习算法。而在自动驾驶领域，诸如行人检测、车道跟踪和交通标志的识别等任务大多使用深度学习算法。在这些任务中，深度学习算法已经接近甚至超过了人类的水平。</p>


![avatar](../image/imageOne/01.jpg)

<center><p class="pColor">图1-1 应用领域 </p></center>

<p class="content">随着神经网络模型越来越深，基于冯诺依曼体系结构的CPU硬件平台，很难满足神经网络的计算需求。冯诺依曼体系结构遵循取指、译指、执行的运行流程，从外部存储设备中读取需要处理的数据，并将执行完毕的结果写入到指定的存储地址中。通常按照冯诺依曼体系结构设计的硬件平台都是以ALU（运算逻辑单元）作为计算核心。CPU由于其具有专用性，需要大量的调度指令操作，这样就会占用大量的片上资源。导致CPU不具有很强并行计算单元能力不能达到专用加速的性能。</p>

<p class="content">GPU是可供选择的另一个平台，GPU压缩了控制逻辑和片上缓存，很好的解决了这个问题。同时GPU使用了大量的SIMD（单指令多数据流）计算单元，使其具有了很强的浮点运算、高带宽、高主频的突出性能，从而极大的提高了GPU的并行计算能力。目前算力较高的GPU处理器使用了很多的流速并行处理计算单元，与CPU处理器较少的核心数量以及线程数相比，GPU通常可以实现非常明显的密集计算加速效果。因此GPU被广泛的用于深度学习的加速训练。但是由于使用过多的计算单元导致GPU处理器的功耗过高，所以GPU处理器一般用于云端服务器和神经网络的训练阶段。</p>

<p class="content">专用的深度学习芯片（ASIC）作为一种解决方案，可以提供更高的算力和更低的功耗。典型代表为GOOGLE公司的TPU。ASIC的参数都及其极端，它有着极高的计算能力和极低的功耗，它的顶峰计算能力与GPU相比毫不逊色，功耗可能会比GPU低100倍。但是，ASIC芯片研发有着极高的成本和风险。ASIC芯片开发需要大量的人力物力投入，开发周期过长，而且失败的风险极大。</p>

<p class="content">开发一款专用ASIC的成本是巨大的，一种折中方案就是开发基于FPGA深度学习的加速器。FPGA 凭借它的高并行计算、低功耗和可重复配置的特点在实现神经网络的多种加速器设计中显示了优异的特性。在性能和功耗上，FPGA可以提供接近ASIC的性能和功耗。在灵活性上，又可以具有和CPU接近的可配置性。在多样性上，FPGA解决方案可覆盖从低到高不同的应用场景下的优化方案。</p>

<p class="content">图1-2给出了CPU、GPU、FPGA和ASIC这四种芯片的架构。表1-1则给出了四种芯片详细的优缺点比较。</p>

![avatar](../image/imageOne/02.jpg)

<center><p class="pColor">图1-2 不同架构的比较</p></center>

![avatar](../image/imageOne/04.jpg)

<center><p class="pColor">表1-1 4种芯片的比较</p></center>




